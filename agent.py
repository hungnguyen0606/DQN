import tensorflow as tf
import numpy as np
import q_estimator
from memory import replay_memory
from collections import namedtuple

collection_dict = namedtuple('collection_dict', ['lstate', 'laction', 'lreward', 'lnext_state', 'lterminal'])
model_param = namedtuple('model_settings', ['lr', 'lr_decay', 'epsilon', 'epsilon_decay', 'decay_time', 'gamma'])


class agent:
    def __init__(self, environment, setting):
        self.env = environment

        self.memory = replay_memory()
        self.source_net = q_estimator.QEstimator(self.env.sample_action, self.env.get_state_size(), self.env.get_num_actions(), setting.lr, 'SourceNet')
        self.target_net = q_estimator.QEstimator(self.env.sample_action, self.env.get_state_size(), self.env.get_num_actions(), setting.lr, 'TargetNet')

        self.gamma = 0.99

        # creating 'update target' ops
        s_vars = [t for t in tf.trainable_variables() if t.name.startswith(self.source_net.scope)]
        t_vars = [t for t in tf.trainable_variables() if t.name.startswith(self.target_net.scope)]
        s_vars = sorted(s_vars, key=lambda t: t.name)
        t_vars = sorted(t_vars, key=lambda t: t.name)
        self.update_ops = []
        for s, t in zip(s_vars, t_vars):
            self.update_ops.append(t.assign(s))
        #----

        self.saver = tf.train.Saver()
        self.init_ops = tf.global_variables_initializer()
        self.sess = tf.Session()

        self.sess.run(self.init_ops)

        # self.rewards = tf.placeholder(tf.float32, [None, 1], name='rewards')
        # self.inputs = np.zeros([0]) #
        # self.get_target = self.rewards + self.gamma*self.target_net.Q_Value(self.sess, )
        # checkpoint = tf.train.get_checkpoint_state()

    def get_target(self, inputs, rewards, terminals):
        """
        Return the target values generated by target_net for training model.
        target = reward + gamma*max(target_net, state)
        or     = reward (if state is a terminal)
        :param inputs: (BATCH_SIZE * STATE_SIZE) numpy array containing list of states
        :param rewards: (BATCH_SIZE * 1) numpy array containing list of rewards
        :param terminals: (BATCH_SIZE *1) list of terminal sign (0: if state is a terminal, 1: otherwise)
        :return: (BATCH_SIZE * 1) list of target values
        """
        assert np.array(inputs).shape[1] == self.env.get_state_size(), 'Shape of inputs doesn\'t match STATE_SIZE: %d != %d' % (inputs.shape[1], self.env.get_state_size())
        target = rewards + self.gamma*terminals*np.max(self.target_net.Q_Value(self.sess, inputs), axis=1, keepdims=True)

        return target

    def prepare_batch(self):
        batch = self.memory.sample(32)
        if batch.size < 32:
            return None

        lstate = []
        laction = []
        lnext_state = []
        lreward = []
        lterminal = []

        for sample in batch.data:
            lstate.append(sample.state)
            laction.append(sample.action)
            lreward.append(sample.reward)
            lnext_state.append(sample.next_state)
            lterminal.append(sample.terminal)

        lstate = np.array(lstate).reshape(-1, self.env.get_state_size())
        # the action list will be mapped to one hot vectors later, so you don't need to change the shape of it.
        # laction = ...
        lnext_state = np.array(lnext_state).reshape(-1, self.env.get_state_size())
        lreward = np.array(lreward).reshape(-1, 1)
        lterminal = np.array(lterminal).reshape(-1, 1)

        return collection_dict(lstate=lstate,
                               laction=laction,
                               lreward=lreward,
                               lnext_state=lnext_state,
                               lterminal=lterminal)

    def update_target_by_source(self):
        self.sess.run(self.update_ops)

    def train(self, max_episode):

        for episode in range(max_episode):
            state = self.env.reset_environment()
            print(state)
            while True:
                action = self.source_net.predict(self.sess, state)
                next_state, reward, done = self.env.step(action)
                self.memory.add(state, action, reward, next_state, done)

                # prepare data
                data = self.prepare_batch()
                if data is not None:
                    target = self.get_target(data.lstate, data.lreward, data.lterminal)
                    # print(self.source_net.train())
                    self.source_net.train(self.sess, data.lstate, data.laction, target)
                state = next_state
                if done:
                    break
            self.saver.save(self.sess, './network' + '-dqn',
                            global_step=episode)
            pass
        pass


if __name__ == '__main__':
    print('Testing agent')
    from problem import MountainProblem
    prob = MountainProblem('./testing')
    # model_param = namedtuple('model_settings', ['lr', 'lr_decay', 'epsilon', 'epsilon_decay', 'decay_time', 'gamma'])
    setting = model_param(0.1, 0.99, 0.2, 0.99, 100, 0.9)
    my_agent = agent(prob, setting)
    my_agent.train(20)