import tensorflow as tf
import numpy as np
from . import q_estimator
from .memory import replay_memory
from collections import namedtuple
import os

collection_dict = namedtuple('collection_dict', ['lstate', 'laction', 'lreward', 'lnext_state', 'lterminal'])
model_param_ = namedtuple('model_settings', ['lr', 'lr_decay',
                                             'epsilon', 'epsilon_decay',
                                             'gamma', 'freeze_time', 'load_path', 'save_path', 'save_time'])

batch_size = 8


class ModelParam(model_param_):
    """
    :param lr: learning rate
    :param lr_decay: decay value for learning rate
    :param lr_decay_step: decay time for lr
    :param epsilon: epsilon value for greedy
    :param epsilon_decay: decay value for epsilon
    :param epsilon_decay_step: decay time for epsilon
    :param gamma: discount value
    :param load_path: path for loading the old agent
    :param save_path: path for saving the current agent
    """
    pass


class Agent:
    def __init__(self, environment, model_setting):
        self.env = environment
        self.setting = model_setting
        self.memory = replay_memory()
        self.source_net = q_estimator.QEstimator(self.env.sample_action, self.env.get_state_size(), self.env.get_num_actions(), 'SourceNet')
        self.target_net = q_estimator.QEstimator(self.env.sample_action, self.env.get_state_size(), self.env.get_num_actions(), 'TargetNet')

        self.global_step = tf.Variable(0, dtype=tf.int32, name='global_step')
        # creating 'update target' ops
        s_vars = [t for t in tf.trainable_variables() if t.name.startswith(self.source_net.scope)]
        t_vars = [t for t in tf.trainable_variables() if t.name.startswith(self.target_net.scope)]
        s_vars = sorted(s_vars, key=lambda t: t.name)
        t_vars = sorted(t_vars, key=lambda t: t.name)
        self.update_ops = []
        for s, t in zip(s_vars, t_vars):
            self.update_ops.append(t.assign(s))
        #----

        self.saver = tf.train.Saver(max_to_keep=10000)
        self.init_ops = tf.global_variables_initializer()
        self.sess = tf.Session()

        self.sess.run(self.init_ops)
        # self.rewards = tf.placeholder(tf.float32, [None, 1], name='rewards')
        # self.inputs = np.zeros([0]) #
        # self.get_target = self.rewards + self.setting.gamma*self.target_net.Q_Value(self.sess, )
        checkpoint = tf.train.get_checkpoint_state(os.path.join(self.setting.load_path, 'models', ''))
        if checkpoint and checkpoint.model_checkpoint_path:
            self.saver.restore(self.sess, checkpoint.model_checkpoint_path)
            print("Successfully loaded old agent at: ", checkpoint.model_checkpoint_path)
        else:
            print("Unable to find the old agent.")

    def get_target(self, inputs, rewards, terminals):
        """
        Return the target values generated by target_net for training model.
        target = reward + gamma*max(target_net, state)
        or     = reward (if state is a terminal)

        :param inputs: (BATCH_SIZE * STATE_SIZE) numpy array containing list of next_states
        :param rewards: (BATCH_SIZE * 1) numpy array containing list of rewards
        :param terminals: (BATCH_SIZE *1) list of terminal sign (0: if state is a terminal, 1: otherwise)
        :return: (BATCH_SIZE * 1) list of target values
        """
        assert np.array(inputs).shape[1] == self.env.get_state_size(), \
            'Shape of inputs doesn\'t match STATE_SIZE: %d != %d' %\
            (inputs.shape[1], self.env.get_state_size())
        target = rewards + self.setting.gamma*terminals*np.max(self.target_net.Q_Value(self.sess, inputs), axis=1, keepdims=True)

        return target

    def prepare_batch(self):
        batch = self.memory.sample(batch_size)
        if batch.size < batch_size:
            return None

        lstate = []
        laction = []
        lnext_state = []
        lreward = []
        lterminal = []

        for sample in batch.data:
            lstate.append(sample.state)
            laction.append(sample.action)
            lreward.append(sample.reward)
            lnext_state.append(sample.next_state)
            lterminal.append(sample.terminal)

        lstate = np.array(lstate).reshape(-1, self.env.get_state_size())
        # the action list will be mapped to one hot vectors later,
        # so you don't need to change the shape of it.
        # laction = ...
        lnext_state = np.array(lnext_state).reshape(-1, self.env.get_state_size())
        lreward = np.array(lreward).reshape(-1, 1)
        lterminal = np.array(lterminal).reshape(-1, 1)

        return collection_dict(lstate=lstate,
                               laction=laction,
                               lreward=lreward,
                               lnext_state=lnext_state,
                               lterminal=lterminal)

    def update_target_by_source(self, global_step):
        self.sess.run(self.update_ops)

    def get_lr(self, global_step):
        return self.setting.lr*np.exp(-self.setting.lr_decay*global_step)

    def get_eps(self, global_step):
        return self.setting.epsilon*np.exp(-self.setting.epsilon_decay*global_step)

    def run_step(self, state, episode=0):
        """
        Take action, add transition to memory
        :param state: current state of the agent
        :param episode: numerical order of episode - for calculate epsilon
        :return: next state, reward, terminal signal
        """
        # choose action using epsilon-greedy policy
        action = self.source_net.epsilon_predict(self.sess, state, self.get_eps(episode))
        # take action & observe the environment
        next_state, reward, done = self.env.step(action)
        # add transition to replay memory
        self.memory.add(state, action, reward, next_state, done)

        return next_state, reward, done

    def full_update(self, gb_step):
        self.update_target_by_source(gb_step)
        pass

    def run_episode(self, episode, max_step = 15000):
        """

        :param episode: number of current episode
        :return: list of reward, list of loss
        """
        state = self.env.reset_environment()
        lreward = []
        lloss = []
        total_reward = 0

        for _ in range(max_step):
            gb_step = self.sess.run(self.global_step)
            next_state, reward, done = self.run_step(state, gb_step)
            total_reward += reward
            state = next_state

            # prepare data, compute & apply gradient
            data = self.prepare_batch()

            target = self.get_target(data.lnext_state, data.lreward, data.lterminal)
            # print(self.source_net.train())
            _loss = self.source_net.train(self.sess, data.lstate, data.laction, target,
                                        self.get_lr(gb_step))


            self.full_update(gb_step)
            self.sess.run(self.global_step.assign(tf.add(self.global_step, 1), 'update_global_step'))
            if done:
                break

        return total_reward

    def init_memory(self):
        """
        Populate the memory for training
        :return:
        """
        # populate memory
        state = self.env.reset_environment()
        for i in range(batch_size):
            state, _, done = self.run_step(state)
            if done:
                state = self.env.reset_environment()
                break

    def train(self, max_episode):
        """
        Train agent to predict action. If you override this method, remember to
        populate the memory before training by using init_memory method
        :param max_episode: number of episodes used to train
        :return:
        """

        self.init_memory()
        for episode in range(max_episode):
            gb_step = self.sess.run(self.global_step)

            # this is important, please do not remove when override this method
            total_reward = self.run_episode(episode, 15000)

            # ---------------------------------------------------------------
            # you can remove this prompt
            # ---------------------------------------------------------------



